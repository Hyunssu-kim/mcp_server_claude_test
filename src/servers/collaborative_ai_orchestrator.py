#!/usr/bin/env python3
"""
Collaborative AI Orchestrator MCP Server
Geminiì™€ Claudeê°€ ì„œë¡œ í˜‘ì—…í•˜ê³  í† ë¡ í•˜ì—¬ ìµœê³ ì˜ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ì‹œìŠ¤í…œ
"""
import asyncio
import json
import sys
from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import logging

# MCP ì„œë²„ë¥¼ ìœ„í•œ ê¸°ë³¸ imports
try:
    from mcp.server import Server
    from mcp.server.models import InitializationOptions
    from mcp.server.stdio import stdio_server
    from mcp.types import (
        CallToolRequest,
        CallToolResult,
        ListToolsRequest,
        ListToolsResult,
        TextContent,
        Tool,
    )
except ImportError:
    print("MCP ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install mcp", file=sys.stderr)
    sys.exit(1)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    stream=sys.stderr
)
logger = logging.getLogger(__name__)

class WorkflowStage(Enum):
    INITIAL_DISCUSSION = "initial_discussion"
    DRAFT_CREATION = "draft_creation"
    PEER_REVIEW = "peer_review"
    IMPROVEMENT = "improvement"
    FINAL_REVIEW = "final_review"
    COMPLETION = "completion"

@dataclass
class CollaborationMessage:
    from_ai: str
    to_ai: str
    stage: WorkflowStage
    content: str
    context: Dict[str, Any]
    timestamp: float

@dataclass
class CollaborationResult:
    task_description: str
    workflow_stages: List[Dict[str, Any]]
    final_result: str
    quality_score: float
    participating_ais: List[str]
    total_iterations: int
    collaboration_summary: str

class CLIExecutor:
    """ê¸°ì¡´ gemini/claude CLI ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ëŠ” í´ë˜ìŠ¤"""
    
    async def execute_gemini(self, prompt: str) -> Dict[str, Any]:
        """Gemini CLI ì‹¤í–‰"""
        try:
            cmd = ["gemini", prompt]
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                text=True
            )
            
            stdout, stderr = await process.communicate()
            
            return {
                "success": process.returncode == 0,
                "result": stdout.strip() if process.returncode == 0 else "",
                "error": stderr.strip() if process.returncode != 0 else None,
                "ai": "gemini"
            }
                
        except Exception as e:
            return {
                "success": False,
                "result": "",
                "error": f"CLI ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}",
                "ai": "gemini"
            }
    
    async def execute_claude(self, prompt: str) -> Dict[str, Any]:
        """Claude CLI ì‹¤í–‰"""
        try:
            cmd = ["claude", prompt]
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                text=True
            )
            
            stdout, stderr = await process.communicate()
            
            return {
                "success": process.returncode == 0,
                "result": stdout.strip() if process.returncode == 0 else "",
                "error": stderr.strip() if process.returncode != 0 else None,
                "ai": "claude"
            }
                
        except Exception as e:
            return {
                "success": False,
                "result": "",
                "error": f"CLI ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}",
                "ai": "claude"
            }

class CollaborativeWorkflow:
    """ë‘ AIê°€ í˜‘ì—…í•˜ëŠ” ì›Œí¬í”Œë¡œìš° ê´€ë¦¬"""
    
    def __init__(self, cli_executor: CLIExecutor):
        self.cli_executor = cli_executor
        self.conversation_history: List[CollaborationMessage] = []
        self.current_stage = WorkflowStage.INITIAL_DISCUSSION
        
    async def start_collaboration(self, task_description: str) -> CollaborationResult:
        """í˜‘ì—… ì›Œí¬í”Œë¡œìš° ì‹œì‘"""
        self.conversation_history = []
        self.current_stage = WorkflowStage.INITIAL_DISCUSSION
        
        logger.info(f"í˜‘ì—… ì‹œì‘: {task_description}")
        
        # 1ë‹¨ê³„: ì´ˆê¸° í† ë¡  - ë‘ AIê°€ ì‘ì—…ì— ëŒ€í•´ ë…¼ì˜
        discussion_result = await self._initial_discussion(task_description)
        
        # 2ë‹¨ê³„: ì´ˆì•ˆ ì‘ì„± - ë” ì í•©í•œ AIê°€ ì´ˆì•ˆ ì‘ì„±
        draft_result = await self._create_draft(task_description, discussion_result)
        
        # 3ë‹¨ê³„: ë™ë£Œ ê²€í†  - ë‹¤ë¥¸ AIê°€ ê²€í†  ë° í”¼ë“œë°±
        review_result = await self._peer_review(task_description, draft_result)
        
        # 4ë‹¨ê³„: ê°œì„  - í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ê°œì„ 
        improved_result = await self._improve_result(task_description, draft_result, review_result)
        
        # 5ë‹¨ê³„: ìµœì¢… ê²€í†  - ì–‘ìª½ì´ ìµœì¢… ê²€í† 
        final_result = await self._final_review(task_description, improved_result)
        
        # 6ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ ë° ìš”ì•½
        quality_score = await self._evaluate_quality(task_description, final_result)
        
        return CollaborationResult(
            task_description=task_description,
            workflow_stages=self._get_workflow_summary(),
            final_result=final_result,
            quality_score=quality_score,
            participating_ais=["gemini", "claude"],
            total_iterations=len(self.conversation_history),
            collaboration_summary=self._generate_collaboration_summary()
        )
    
    async def _initial_discussion(self, task: str) -> Dict[str, str]:
        """1ë‹¨ê³„: ì´ˆê¸° í† ë¡ """
        self.current_stage = WorkflowStage.INITIAL_DISCUSSION
        logger.info(f"ğŸ¯ 1ë‹¨ê³„: ì´ˆê¸° í† ë¡  ì‹œì‘ - {task}")
        
        # Geminiì—ê²Œ ë¨¼ì € ì‘ì—… ë¶„ì„ ìš”ì²­
        gemini_prompt = f"""
ì‘ì—…: {task}

ì´ ì‘ì—…ì— ëŒ€í•´ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì‘ì—…ì˜ í•µì‹¬ ìš”êµ¬ì‚¬í•­
2. ì–´ë ¤ìš´ ì ì´ë‚˜ ì£¼ì˜ì‚¬í•­
3. Gemini vs Claude ì¤‘ ëˆ„ê°€ ë” ì í•©í•œì§€ì™€ ì´ìœ 
4. í˜‘ì—… ì‹œ ì–´ë–¤ ì—­í•  ë¶„ë‹´ì´ ì¢‹ì„ì§€

ì‘ë‹µ í˜•ì‹: JSON
{{
    "analysis": "ì‘ì—… ë¶„ì„",
    "challenges": "ì–´ë ¤ìš´ ì ",
    "better_ai": "gemini ë˜ëŠ” claude",
    "reason": "ì´ìœ ",
    "collaboration_plan": "í˜‘ì—… ê³„íš"
}}
"""
        
        logger.info("ğŸ’­ Geminiì—ê²Œ ì‘ì—… ë¶„ì„ ìš”ì²­ ì¤‘...")
        gemini_result = await self.cli_executor.execute_gemini(gemini_prompt)
        logger.info(f"âœ… Gemini ë¶„ì„ ì™„ë£Œ: {gemini_result['result'][:100]}...")
        
        # Claudeì—ê²Œ Geminiì˜ ë¶„ì„ì— ëŒ€í•œ ì˜ê²¬ ìš”ì²­
        logger.info("ğŸ” Claudeì—ê²Œ Gemini ë¶„ì„ ê²€í†  ìš”ì²­ ì¤‘...")
        claude_prompt = f"""
ì‘ì—…: {task}

Geminiì˜ ë¶„ì„:
{gemini_result['result']}

Geminiì˜ ë¶„ì„ì— ëŒ€í•œ ë‹¹ì‹ ì˜ ì˜ê²¬ê³¼ ì¶”ê°€ ì œì•ˆì„ í•´ì£¼ì„¸ìš”:
1. Gemini ë¶„ì„ì— ë™ì˜í•˜ëŠ”ì§€
2. ë‹¤ë¥¸ ê´€ì ì´ë‚˜ ë†“ì¹œ ë¶€ë¶„
3. ë” ë‚˜ì€ í˜‘ì—… ë°©ì•ˆ
4. ìµœì¢… ì—­í•  ë¶„ë‹´ ì œì•ˆ

ì‘ë‹µ í˜•ì‹: JSON
{{
    "agreement_level": "1-10 ì ìˆ˜",
    "additional_insights": "ì¶”ê°€ í†µì°°",
    "collaboration_suggestion": "í˜‘ì—… ì œì•ˆ",
    "role_assignment": "ìµœì¢… ì—­í•  ë¶„ë‹´"
}}
"""
        
        claude_result = await self.cli_executor.execute_claude(claude_prompt)
        logger.info(f"âœ… Claude ê²€í†  ì™„ë£Œ: {claude_result['result'][:100]}...")
        
        return {
            "gemini_analysis": gemini_result['result'],
            "claude_feedback": claude_result['result']
        }
    
    async def _create_draft(self, task: str, discussion: Dict[str, str]) -> str:
        """2ë‹¨ê³„: ì´ˆì•ˆ ì‘ì„±"""
        self.current_stage = WorkflowStage.DRAFT_CREATION
        logger.info("âœï¸ 2ë‹¨ê³„: ì´ˆì•ˆ ì‘ì„± ì‹œì‘")
        
        # í† ë¡  ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëˆ„ê°€ ì´ˆì•ˆì„ ì‘ì„±í• ì§€ ê²°ì •
        decision_prompt = f"""
ì‘ì—…: {task}

í† ë¡  ê²°ê³¼:
- Gemini ë¶„ì„: {discussion['gemini_analysis']}
- Claude í”¼ë“œë°±: {discussion['claude_feedback']}

ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëˆ„ê°€ ì´ˆì•ˆì„ ì‘ì„±í•´ì•¼ í• ì§€ "gemini" ë˜ëŠ” "claude"ë¡œë§Œ ë‹µí•˜ì„¸ìš”.
"""
        
        decision_result = await self.cli_executor.execute_gemini(decision_prompt)
        primary_ai = "claude" if "claude" in decision_result['result'].lower() else "gemini"
        logger.info(f"ğŸ¯ {primary_ai.upper()}ê°€ ì´ˆì•ˆ ì‘ì„±ìœ¼ë¡œ ì„ íƒë¨")
        
        # ì„ íƒëœ AIê°€ ì´ˆì•ˆ ì‘ì„±
        draft_prompt = f"""
ì‘ì—…: {task}

í˜‘ì—… í† ë¡  ê²°ê³¼:
{discussion['gemini_analysis']}
{discussion['claude_feedback']}

ì´ í† ë¡ ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”. ìµœê³  í’ˆì§ˆì˜ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
"""
        
        logger.info(f"ğŸ“ {primary_ai.upper()}ì—ê²Œ ì´ˆì•ˆ ì‘ì„± ìš”ì²­ ì¤‘...")
        if primary_ai == "gemini":
            result = await self.cli_executor.execute_gemini(draft_prompt)
        else:
            result = await self.cli_executor.execute_claude(draft_prompt)
        logger.info(f"âœ… ì´ˆì•ˆ ì‘ì„± ì™„ë£Œ: {result['result'][:100]}...")
        
        return result['result']
    
    async def _peer_review(self, task: str, draft: str) -> str:
        """3ë‹¨ê³„: ë™ë£Œ ê²€í† """
        self.current_stage = WorkflowStage.PEER_REVIEW
        logger.info("ğŸ” 3ë‹¨ê³„: ë™ë£Œ ê²€í†  ì‹œì‘")
        
        # ì´ˆì•ˆì„ ì‘ì„±í•˜ì§€ ì•Šì€ AIê°€ ê²€í† 
        review_prompt = f"""
ì›ë˜ ì‘ì—…: {task}

ë™ë£Œê°€ ì‘ì„±í•œ ê²°ê³¼:
{draft}

ì´ ê²°ê³¼ë¥¼ ê²€í† í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”:
1. ì˜ëœ ì 
2. ê°œì„ ì´ í•„ìš”í•œ ì 
3. êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆ
4. ë†“ì¹œ ë¶€ë¶„ì´ë‚˜ ì¶”ê°€í•  ë‚´ìš©
5. ì „ì²´ì ì¸ í’ˆì§ˆ í‰ê°€ (1-10ì )

ê±´ì„¤ì ì´ê³  êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        
        # ë‘ AI ëª¨ë‘ì—ê²Œ ê²€í†  ìš”ì²­ (ë‹¤ì–‘í•œ ê´€ì )
        logger.info("ğŸ‘¥ ë‘ AI ëª¨ë‘ì—ê²Œ ê²€í†  ìš”ì²­ ì¤‘...")
        gemini_review = await self.cli_executor.execute_gemini(review_prompt)
        claude_review = await self.cli_executor.execute_claude(review_prompt)
        logger.info("âœ… ì–‘ìª½ AI ê²€í†  ì™„ë£Œ")
        
        return f"Gemini ê²€í† :\n{gemini_review['result']}\n\nClaude ê²€í† :\n{claude_review['result']}"
    
    async def _improve_result(self, task: str, draft: str, reviews: str) -> str:
        """4ë‹¨ê³„: ê°œì„ """
        self.current_stage = WorkflowStage.IMPROVEMENT
        logger.info("ğŸš€ 4ë‹¨ê³„: í”¼ë“œë°± ê¸°ë°˜ ê°œì„  ì‹œì‘")
        
        improvement_prompt = f"""
ì›ë˜ ì‘ì—…: {task}

ì´ˆì•ˆ:
{draft}

ê²€í†  í”¼ë“œë°±:
{reviews}

í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ê²°ê³¼ë¥¼ ê°œì„ í•´ì£¼ì„¸ìš”. ëª¨ë“  ì§€ì ì‚¬í•­ì„ ê³ ë ¤í•˜ì—¬ ë” ë‚˜ì€ ë²„ì „ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
"""
        
        # ë‘ AIê°€ ê°ê° ê°œì„ ì•ˆ ì œì‹œ
        logger.info("ğŸ’¡ ë‘ AIê°€ ê°ê° ê°œì„ ì•ˆ ì œì‹œ ì¤‘...")
        gemini_improved = await self.cli_executor.execute_gemini(improvement_prompt)
        claude_improved = await self.cli_executor.execute_claude(improvement_prompt)
        logger.info("âœ… ì–‘ìª½ ê°œì„ ì•ˆ ì™„ì„±")
        
        # ë‘ ê°œì„ ì•ˆì„ ë¹„êµí•˜ì—¬ ìµœê³  ì„ íƒ
        comparison_prompt = f"""
ì›ë˜ ì‘ì—…: {task}

Gemini ê°œì„ ì•ˆ:
{gemini_improved['result']}

Claude ê°œì„ ì•ˆ:
{claude_improved['result']}

ë‘ ê°œì„ ì•ˆì„ ë¹„êµí•˜ê³  ë” ë‚˜ì€ ê²ƒì„ ì„ íƒí•˜ê±°ë‚˜, ë‘ ê°œì˜ ì¥ì ì„ ê²°í•©í•œ ìµœì¢… ë²„ì „ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
"""
        
        logger.info("âš–ï¸ ê°œì„ ì•ˆ ë¹„êµ ë° ìµœì¢… ì„ íƒ ì¤‘...")
        final_improved = await self.cli_executor.execute_gemini(comparison_prompt)
        logger.info("âœ… ìµœì¢… ê°œì„  ë²„ì „ ì™„ì„±")
        return final_improved['result']
    
    async def _final_review(self, task: str, improved_result: str) -> str:
        """5ë‹¨ê³„: ìµœì¢… ê²€í† """
        self.current_stage = WorkflowStage.FINAL_REVIEW
        logger.info("âœ… 5ë‹¨ê³„: ìµœì¢… ê²€í†  ì‹œì‘")
        
        final_check_prompt = f"""
ì›ë˜ ì‘ì—…: {task}

ìµœì¢… ê²°ê³¼:
{improved_result}

ì´ê²ƒì´ ìµœì¢… ê²°ê³¼ì…ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ê²€í† í•˜ê³  í•„ìš”í•˜ë©´ ë¯¸ì„¸ ì¡°ì •í•´ì£¼ì„¸ìš”:
1. ì‘ì—… ìš”êµ¬ì‚¬í•­ì„ ëª¨ë‘ ì¶©ì¡±í–ˆëŠ”ì§€ í™•ì¸
2. í’ˆì§ˆì´ ìµœê³  ìˆ˜ì¤€ì¸ì§€ í™•ì¸
3. í•„ìš”í•˜ë©´ ìµœì¢… ë‹¤ë“¬ê¸°

ì™„ë²½í•œ ìµœì¢… ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        
        # ë‘ AIê°€ ìµœì¢… ê²€í† 
        logger.info("ğŸ¯ ì–‘ìª½ AIì˜ ìµœì¢… ê²€í†  ì§„í–‰ ì¤‘...")
        gemini_final = await self.cli_executor.execute_gemini(final_check_prompt)
        claude_final = await self.cli_executor.execute_claude(final_check_prompt)
        logger.info("âœ… ìµœì¢… ê²€í†  ì™„ë£Œ")
        
        # ë” ë‚˜ì€ ìµœì¢… ë²„ì „ ì„ íƒ
        selection_prompt = f"""
Gemini ìµœì¢… ë²„ì „:
{gemini_final['result']}

Claude ìµœì¢… ë²„ì „:
{claude_final['result']}

ë” ë‚˜ì€ ìµœì¢… ë²„ì „ì„ ì„ íƒí•˜ê±°ë‚˜ ë‘ ë²„ì „ì˜ ì¥ì ì„ ê²°í•©í•´ì£¼ì„¸ìš”.
"""
        
        logger.info("ğŸ† ìµœì¢… ë²„ì „ ì„ íƒ ì¤‘...")
        final_result = await self.cli_executor.execute_claude(selection_prompt)
        logger.info("ğŸ‰ ìµœì¢… ê²°ê³¼ ì™„ì„±!")
        return final_result['result']
    
    async def _evaluate_quality(self, task: str, result: str) -> float:
        """í’ˆì§ˆ í‰ê°€"""
        logger.info("ğŸ“Š 6ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ ì‹œì‘")
        evaluation_prompt = f"""
ì‘ì—…: {task}
ê²°ê³¼: {result}

ì´ ê²°ê³¼ì˜ í’ˆì§ˆì„ 1-10ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”. í‰ê°€ ê¸°ì¤€:
1. ì‘ì—… ìš”êµ¬ì‚¬í•­ ì¶©ì¡±ë„
2. ê²°ê³¼ì˜ ì •í™•ì„±
3. ì™„ì„±ë„
4. ì°½ì˜ì„±/ìœ ìš©ì„±

ì ìˆ˜ë§Œ ìˆ«ìë¡œ ë‹µí•˜ì„¸ìš”.
"""
        
        # ë‘ AIì˜ í‰ê°€ í‰ê· 
        logger.info("ğŸ¯ ì–‘ìª½ AIì˜ í’ˆì§ˆ í‰ê°€ ì§„í–‰ ì¤‘...")
        gemini_score = await self.cli_executor.execute_gemini(evaluation_prompt)
        claude_score = await self.cli_executor.execute_claude(evaluation_prompt)
        
        try:
            g_score = float(gemini_score['result'].strip())
            c_score = float(claude_score['result'].strip())
            final_score = (g_score + c_score) / 2
            logger.info(f"ğŸ“Š í’ˆì§ˆ ì ìˆ˜: Gemini({g_score}) + Claude({c_score}) = í‰ê·  {final_score}")
            return final_score
        except:
            logger.warning("âš ï¸ í’ˆì§ˆ ì ìˆ˜ íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ 8.0 ì‚¬ìš©")
            return 8.0  # ê¸°ë³¸ê°’
    
    def _get_workflow_summary(self) -> List[Dict[str, Any]]:
        """ì›Œí¬í”Œë¡œìš° ìš”ì•½"""
        return [
            {"stage": "initial_discussion", "description": "ë‘ AIê°€ ì‘ì—…ì— ëŒ€í•´ í† ë¡ "},
            {"stage": "draft_creation", "description": "ì í•©í•œ AIê°€ ì´ˆì•ˆ ì‘ì„±"},
            {"stage": "peer_review", "description": "ë™ë£Œ AIê°€ ê²€í†  ë° í”¼ë“œë°±"},
            {"stage": "improvement", "description": "í”¼ë“œë°± ë°”íƒ•ìœ¼ë¡œ ê°œì„ "},
            {"stage": "final_review", "description": "ìµœì¢… ê²€í†  ë° ë‹¤ë“¬ê¸°"},
            {"stage": "quality_evaluation", "description": "í’ˆì§ˆ í‰ê°€"}
        ]
    
    def _generate_collaboration_summary(self) -> str:
        """í˜‘ì—… ìš”ì•½"""
        return f"Geminiì™€ Claudeê°€ {len(self.conversation_history)}íšŒì˜ ìƒí˜¸ì‘ìš©ì„ í†µí•´ í˜‘ì—…í•˜ì—¬ ìµœê³  í’ˆì§ˆì˜ ê²°ê³¼ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤."

class CollaborativeAIOrchestrator:
    """í˜‘ì—… AI ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.cli_executor = CLIExecutor()
        self.workflow = CollaborativeWorkflow(self.cli_executor)
        self.collaboration_history: List[CollaborationResult] = []
    
    async def execute_collaborative_task(self, task_description: str) -> CollaborationResult:
        """í˜‘ì—… ì‘ì—… ì‹¤í–‰"""
        logger.info(f"í˜‘ì—… ì‘ì—… ì‹œì‘: {task_description}")
        
        result = await self.workflow.start_collaboration(task_description)
        self.collaboration_history.append(result)
        
        return result
    
    async def quick_discussion(self, topic: str) -> Dict[str, str]:
        """ê°„ë‹¨í•œ í† ë¡  (ë¹ ë¥¸ í˜‘ì—…)"""
        discussion_prompt = f"ì´ ì£¼ì œì— ëŒ€í•´ ê°„ë‹¨íˆ ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”: {topic}"
        
        gemini_result = await self.cli_executor.execute_gemini(discussion_prompt)
        claude_result = await self.cli_executor.execute_claude(discussion_prompt)
        
        return {
            "topic": topic,
            "gemini_opinion": gemini_result['result'],
            "claude_opinion": claude_result['result']
        }
    
    async def compare_approaches(self, task: str) -> Dict[str, str]:
        """ë‘ AIì˜ ì ‘ê·¼ë²• ë¹„êµ"""
        comparison_prompt = f"ì´ ì‘ì—…ì— ëŒ€í•œ ë‹¹ì‹ ì˜ ì ‘ê·¼ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”: {task}"
        
        gemini_approach = await self.cli_executor.execute_gemini(comparison_prompt)
        claude_approach = await self.cli_executor.execute_claude(comparison_prompt)
        
        # ì ‘ê·¼ë²• ë¹„êµ ë¶„ì„
        analysis_prompt = f"""
ì‘ì—…: {task}

Gemini ì ‘ê·¼ë²•:
{gemini_approach['result']}

Claude ì ‘ê·¼ë²•:
{claude_approach['result']}

ë‘ ì ‘ê·¼ë²•ì˜ ì¥ë‹¨ì ì„ ë¹„êµí•˜ê³  ìµœì ì˜ ë°©ë²•ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.
"""
        
        analysis = await self.cli_executor.execute_gemini(analysis_prompt)
        
        return {
            "task": task,
            "gemini_approach": gemini_approach['result'],
            "claude_approach": claude_approach['result'],
            "comparison_analysis": analysis['result']
        }
    
    def get_collaboration_stats(self) -> Dict[str, Any]:
        """í˜‘ì—… í†µê³„"""
        total_collaborations = len(self.collaboration_history)
        if total_collaborations == 0:
            return {"message": "ì•„ì§ í˜‘ì—… ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        avg_quality = sum(c.quality_score for c in self.collaboration_history) / total_collaborations
        avg_iterations = sum(c.total_iterations for c in self.collaboration_history) / total_collaborations
        
        return {
            "total_collaborations": total_collaborations,
            "average_quality_score": round(avg_quality, 2),
            "average_iterations": round(avg_iterations, 1),
            "best_collaboration": max(self.collaboration_history, key=lambda x: x.quality_score).task_description
        }

# MCP ì„œë²„ ì„¤ì •
server = Server("collaborative-ai-orchestrator")
orchestrator = CollaborativeAIOrchestrator()

@server.list_tools()
async def handle_list_tools() -> ListToolsResult:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤ ë°˜í™˜"""
    return ListToolsResult(
        tools=[
            Tool(
                name="collaborative_task",
                description="Geminiì™€ Claudeê°€ í˜‘ì—…í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤ (ì „ì²´ ì›Œí¬í”Œë¡œìš°)",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "task": {
                            "type": "string",
                            "description": "ìˆ˜í–‰í•  ì‘ì—… ì„¤ëª…"
                        }
                    },
                    "required": ["task"]
                }
            ),
            Tool(
                name="quick_discussion",
                description="íŠ¹ì • ì£¼ì œì— ëŒ€í•´ ë‘ AIê°€ ë¹ ë¥´ê²Œ í† ë¡ í•©ë‹ˆë‹¤",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "topic": {
                            "type": "string",
                            "description": "í† ë¡ í•  ì£¼ì œ"
                        }
                    },
                    "required": ["topic"]
                }
            ),
            Tool(
                name="compare_approaches",
                description="íŠ¹ì • ì‘ì—…ì— ëŒ€í•œ ë‘ AIì˜ ì ‘ê·¼ë²•ì„ ë¹„êµí•©ë‹ˆë‹¤",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "task": {
                            "type": "string",
                            "description": "ë¹„êµí•  ì‘ì—…"
                        }
                    },
                    "required": ["task"]
                }
            ),
            Tool(
                name="get_collaboration_stats",
                description="í˜‘ì—… í†µê³„ ë° ê¸°ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
                inputSchema={
                    "type": "object",
                    "properties": {}
                }
            ),
            Tool(
                name="execute_gemini_direct",
                description="Geminiì—ê²Œ ì§ì ‘ ì‘ì—…ì„ ìš”ì²­í•©ë‹ˆë‹¤",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "prompt": {
                            "type": "string",
                            "description": "Geminiì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸"
                        }
                    },
                    "required": ["prompt"]
                }
            ),
            Tool(
                name="execute_claude_direct",
                description="Claudeì—ê²Œ ì§ì ‘ ì‘ì—…ì„ ìš”ì²­í•©ë‹ˆë‹¤",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "prompt": {
                            "type": "string",
                            "description": "Claudeì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸"
                        }
                    },
                    "required": ["prompt"]
                }
            )
        ]
    )

@server.call_tool()
async def handle_call_tool(request: CallToolRequest) -> CallToolResult:
    """ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""
    
    try:
        if request.name == "collaborative_task":
            task = request.params.get("task", "")
            if not task:
                return CallToolResult(
                    content=[TextContent(type="text", text="ERROR: ì‘ì—… ì„¤ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤")]
                )
            
            result = await orchestrator.execute_collaborative_task(task)
            
            response = {
                "task": result.task_description,
                "final_result": result.final_result,
                "quality_score": result.quality_score,
                "total_iterations": result.total_iterations,
                "workflow_summary": result.workflow_stages,
                "collaboration_summary": result.collaboration_summary
            }
            
            return CallToolResult(
                content=[TextContent(type="text", text=json.dumps(response, ensure_ascii=False, indent=2))]
            )
        
        elif request.name == "quick_discussion":
            topic = request.params.get("topic", "")
            if not topic:
                return CallToolResult(
                    content=[TextContent(type="text", text="ERROR: í† ë¡  ì£¼ì œê°€ í•„ìš”í•©ë‹ˆë‹¤")]
                )
            
            result = await orchestrator.quick_discussion(topic)
            
            return CallToolResult(
                content=[TextContent(type="text", text=json.dumps(result, ensure_ascii=False, indent=2))]
            )
        
        elif request.name == "compare_approaches":
            task = request.params.get("task", "")
            if not task:
                return CallToolResult(
                    content=[TextContent(type="text", text="ERROR: ë¹„êµí•  ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤")]
                )
            
            result = await orchestrator.compare_approaches(task)
            
            return CallToolResult(
                content=[TextContent(type="text", text=json.dumps(result, ensure_ascii=False, indent=2))]
            )
        
        elif request.name == "get_collaboration_stats":
            stats = orchestrator.get_collaboration_stats()
            
            return CallToolResult(
                content=[TextContent(type="text", text=json.dumps(stats, ensure_ascii=False, indent=2))]
            )
        
        elif request.name == "execute_gemini_direct":
            prompt = request.params.get("prompt", "")
            if not prompt:
                return CallToolResult(
                    content=[TextContent(type="text", text="ERROR: í”„ë¡¬í”„íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤")]
                )
            
            result = await orchestrator.cli_executor.execute_gemini(prompt)
            
            return CallToolResult(
                content=[TextContent(type="text", text=json.dumps(result, ensure_ascii=False, indent=2))]
            )
        
        elif request.name == "execute_claude_direct":
            prompt = request.params.get("prompt", "")
            if not prompt:
                return CallToolResult(
                    content=[TextContent(type="text", text="ERROR: í”„ë¡¬í”„íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤")]
                )
            
            result = await orchestrator.cli_executor.execute_claude(prompt)
            
            return CallToolResult(
                content=[TextContent(type="text", text=json.dumps(result, ensure_ascii=False, indent=2))]
            )
        
        else:
            return CallToolResult(
                content=[TextContent(type="text", text=f"ERROR: ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {request.name}")]
            )
    
    except Exception as e:
        logger.error(f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return CallToolResult(
            content=[TextContent(type="text", text=f"ERROR: {str(e)}")]
        )

async def main():
    """MCP ì„œë²„ ì‹¤í–‰"""
    init_options = InitializationOptions(
        server_name="collaborative-ai-orchestrator",
        server_version="2.0.0",
        capabilities={}
    )
    
    try:
        async with stdio_server() as (read_stream, write_stream):
            await server.run(
                read_stream,
                write_stream,
                init_options
            )
    except Exception as e:
        logger.error(f"ì„œë²„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())