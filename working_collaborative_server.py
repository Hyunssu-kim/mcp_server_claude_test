#!/usr/bin/env python3
"""
Working Collaborative AI Orchestrator - ì•ˆì •ì ì¸ ë²„ì „
"""
import asyncio
import json
import sys
import subprocess
from typing import Any, Dict, List, Optional

try:
    from mcp.server import Server
    from mcp.server.models import InitializationOptions
    from mcp.server.stdio import stdio_server
    from mcp.types import (
        CallToolRequest,
        CallToolResult,
        ListToolsRequest,
        ListToolsResult,
        TextContent,
        Tool,
    )
except ImportError:
    print("MCP ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install mcp", file=sys.stderr)
    sys.exit(1)

class SimpleCLIExecutor:
    """ê°„ë‹¨í•œ CLI ì‹¤í–‰ê¸°"""
    
    async def execute_command(self, cmd: List[str]) -> Dict[str, Any]:
        """ëª…ë ¹ì–´ ì‹¤í–‰"""
        try:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                text=True
            )
            
            stdout, stderr = await process.communicate()
            
            return {
                "success": process.returncode == 0,
                "result": stdout.strip() if process.returncode == 0 else "",
                "error": stderr.strip() if process.returncode != 0 else None,
                "command": " ".join(cmd)
            }
                
        except Exception as e:
            return {
                "success": False,
                "result": "",
                "error": f"ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}",
                "command": " ".join(cmd)
            }

class WorkingCollaborativeAI:
    """ì‹¤ì œ ì‘ë™í•˜ëŠ” í˜‘ì—… AI"""
    
    def __init__(self):
        self.cli = SimpleCLIExecutor()
        self.session_count = 0
    
    async def simple_collaboration(self, task: str) -> Dict[str, Any]:
        """ê°„ë‹¨í•œ í˜‘ì—… ìˆ˜í–‰"""
        self.session_count += 1
        
        print(f"ğŸ¤ í˜‘ì—… ì„¸ì…˜ #{self.session_count} ì‹œì‘: {task}", file=sys.stderr)
        
        # 1ë‹¨ê³„: Geminiì—ê²Œ ë¨¼ì € ë¬¼ì–´ë³´ê¸°
        gemini_result = await self.cli.execute_command(["gemini", f"ì´ ì‘ì—…ì— ëŒ€í•´ ë¶„ì„í•´ì£¼ì„¸ìš”: {task}"])
        
        # 2ë‹¨ê³„: Claudeì—ê²Œë„ ë¬¼ì–´ë³´ê¸°  
        claude_result = await self.cli.execute_command(["claude", f"ì´ ì‘ì—…ì— ëŒ€í•´ ë¶„ì„í•´ì£¼ì„¸ìš”: {task}"])
        
        # 3ë‹¨ê³„: ë‘ ê²°ê³¼ ë¹„êµ
        if gemini_result["success"] and claude_result["success"]:
            # ë‘ AI ëª¨ë‘ ì„±ê³µí•œ ê²½ìš°
            comparison_task = f"""
ë‹¤ìŒ ë‘ AIì˜ ë¶„ì„ì„ ë¹„êµí•˜ê³  ìµœì„ ì˜ ë°©ë²•ì„ ì œì•ˆí•´ì£¼ì„¸ìš”:

ì‘ì—…: {task}

Gemini ë¶„ì„:
{gemini_result['result']}

Claude ë¶„ì„:  
{claude_result['result']}

ë‘ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬ ìµœê³ ì˜ ì†”ë£¨ì…˜ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
            final_result = await self.cli.execute_command(["claude", comparison_task])
            
            return {
                "task": task,
                "session_id": self.session_count,
                "workflow": "complete_collaboration",
                "gemini_analysis": gemini_result['result'],
                "claude_analysis": claude_result['result'], 
                "final_solution": final_result['result'] if final_result['success'] else "ìµœì¢… ë¶„ì„ ì‹¤íŒ¨",
                "success": True,
                "message": "ë‘ AIê°€ ì„±ê³µì ìœ¼ë¡œ í˜‘ì—…í–ˆìŠµë‹ˆë‹¤!"
            }
        
        elif gemini_result["success"]:
            # Geminië§Œ ì„±ê³µ
            return {
                "task": task,
                "session_id": self.session_count,
                "workflow": "gemini_only",
                "result": gemini_result['result'],
                "success": True,
                "message": "Geminiê°€ ì‘ì—…ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤."
            }
            
        elif claude_result["success"]:
            # Claudeë§Œ ì„±ê³µ
            return {
                "task": task,
                "session_id": self.session_count,
                "workflow": "claude_only", 
                "result": claude_result['result'],
                "success": True,
                "message": "Claudeê°€ ì‘ì—…ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤."
            }
        
        else:
            # ë‘˜ ë‹¤ ì‹¤íŒ¨
            return {
                "task": task,
                "session_id": self.session_count,
                "workflow": "failed",
                "gemini_error": gemini_result['error'],
                "claude_error": claude_result['error'],
                "success": False,
                "message": "ë‘ AI ëª¨ë‘ ì‘ì—…ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            }
    
    async def quick_chat(self, ai: str, message: str) -> Dict[str, Any]:
        """íŠ¹ì • AIì™€ ë¹ ë¥¸ ëŒ€í™”"""
        if ai.lower() not in ["gemini", "claude"]:
            return {"error": "AIëŠ” 'gemini' ë˜ëŠ” 'claude'ì—¬ì•¼ í•©ë‹ˆë‹¤."}
        
        result = await self.cli.execute_command([ai.lower(), message])
        return {
            "ai": ai,
            "message": message,
            "response": result['result'] if result['success'] else result['error'],
            "success": result['success']
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ë°˜í™˜"""
        return {
            "total_sessions": self.session_count,
            "status": "operational",
            "available_ais": ["gemini", "claude"]
        }

# MCP ì„œë²„ ì„¤ì •
server = Server("working-collaborative-ai")
orchestrator = WorkingCollaborativeAI()

@server.list_tools()
async def handle_list_tools() -> ListToolsResult:
    """ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
    return ListToolsResult(
        tools=[
            Tool(
                name="collaborate",
                description="Geminiì™€ Claudeê°€ í˜‘ì—…í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "task": {
                            "type": "string",
                            "description": "ìˆ˜í–‰í•  ì‘ì—… ì„¤ëª…"
                        }
                    },
                    "required": ["task"]
                }
            ),
            Tool(
                name="chat_with_ai",
                description="íŠ¹ì • AIì™€ ì§ì ‘ ëŒ€í™”í•©ë‹ˆë‹¤",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "ai": {
                            "type": "string",
                            "enum": ["gemini", "claude"],
                            "description": "ëŒ€í™”í•  AI (gemini ë˜ëŠ” claude)"
                        },
                        "message": {
                            "type": "string", 
                            "description": "ì „ë‹¬í•  ë©”ì‹œì§€"
                        }
                    },
                    "required": ["ai", "message"]
                }
            ),
            Tool(
                name="get_stats",
                description="í˜‘ì—… í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
                inputSchema={
                    "type": "object",
                    "properties": {}
                }
            )
        ]
    )

@server.call_tool()
async def handle_call_tool(request: CallToolRequest) -> CallToolResult:
    """ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""
    
    try:
        if request.name == "collaborate":
            task = request.params.get("task", "")
            if not task:
                return CallToolResult(
                    content=[TextContent(type="text", text="âŒ ì‘ì—… ì„¤ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤")]
                )
            
            print(f"ğŸš€ í˜‘ì—… ì‘ì—… ì‹œì‘: {task}", file=sys.stderr)
            result = await orchestrator.simple_collaboration(task)
            
            return CallToolResult(
                content=[TextContent(type="text", text=json.dumps(result, ensure_ascii=False, indent=2))]
            )
        
        elif request.name == "chat_with_ai":
            ai = request.params.get("ai", "")
            message = request.params.get("message", "")
            
            if not ai or not message:
                return CallToolResult(
                    content=[TextContent(type="text", text="âŒ AIì™€ ë©”ì‹œì§€ê°€ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤")]
                )
            
            result = await orchestrator.quick_chat(ai, message)
            
            return CallToolResult(
                content=[TextContent(type="text", text=json.dumps(result, ensure_ascii=False, indent=2))]
            )
        
        elif request.name == "get_stats":
            stats = orchestrator.get_stats()
            
            return CallToolResult(
                content=[TextContent(type="text", text=json.dumps(stats, ensure_ascii=False, indent=2))]
            )
        
        else:
            return CallToolResult(
                content=[TextContent(type="text", text=f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {request.name}")]
            )
    
    except Exception as e:
        print(f"âŒ ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}", file=sys.stderr)
        return CallToolResult(
            content=[TextContent(type="text", text=f"âŒ ì˜¤ë¥˜: {str(e)}")]
        )

async def main():
    """MCP ì„œë²„ ì‹¤í–‰"""
    init_options = InitializationOptions(
        server_name="working-collaborative-ai",
        server_version="1.0.0",
        capabilities={}
    )
    
    print("ğŸ¤ Working Collaborative AI ì„œë²„ ì‹œì‘...", file=sys.stderr)
    
    try:
        async with stdio_server() as (read_stream, write_stream):
            await server.run(
                read_stream,
                write_stream,
                init_options
            )
    except Exception as e:
        print(f"âŒ ì„œë²„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())